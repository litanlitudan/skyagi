

import sys
import time

import audio2face_pb2
import audio2face_pb2_grpc
import grpc
import numpy as np
import soundfile

import os
import openai
import requests
import json
import threading
import logging
import librosa

openai.api_key = ""
PLAYHT_KEY = ""
PLAYHT_ID = ""
model_id = "gpt-3.5-turbo"
morty_id = "s3://voice-cloning-zero-shot/90e078bf-6b88-49b8-92d0-b72bf1b6bbe6/morty-v3/manifest.json"
rick_id = "s3://voice-cloning-zero-shot/e0b30249-a594-4ec6-ab6f-82b52178045e/rick/manifest.json"



def getPromptFromText(previousText):
    tomIndex = previousText.rfind("Rick:")
    jerryIndex = previousText.rfind("Morty:")
    inputText = previousText
    inputCharacter = ""
    inputPersona = ""
    if tomIndex > jerryIndex:
        inputText += "\n\nMorty:"
        inputCharacter = "Morty"
        inputPersona = "You are afraid of uncertainties. You are a teenager. You do not think much before making actions."
    else:
        inputText += "\n\nRick:"
        inputCharacter = "Rick"
        inputPersona = "You are smart. You are knowledgeble. You love adventures. You are reckless."
    prompt = f"""
    You are {inputCharacter}. {inputPersona} Continue the conversation delimited by triple backticks by one more response.
    Stop when the role is changed from Rick to Morty or from Morty to Rick.
    The respond should be based on your characteristics. 
    ```{inputText}```
    """
    print(prompt)
    return (prompt, inputText)

def get_completion(prompt, model):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message["content"]


def getAudioFileFromId(id):
    url = "https://play.ht/api/v1/articleStatus/?transcriptionId="+id

    headers = {
        "accept": "application/json",
        "X-User-Id": PLAYHT_ID,
        "Authorization": PLAYHT_KEY
    }
    
    response = requests.get(url, headers=headers)
    responseData = json.loads(response.text)
    completeStatus = responseData["transcriped"]
    while (completeStatus != True):
        response = requests.get(url, headers=headers)
        responseData = json.loads(response.text)
        completeStatus = responseData["transcriped"]
        print(responseData)
        time.sleep(0.5)
    print(responseData["audioUrl"])
    return responseData["audioUrl"]
    

def downloadFile(audioUrl, fileName):
    r = requests.get(audioUrl[0])
    with open(fileName, "wb") as f:
        f.write(r.content)
    return fileName

def getAudioFromText(text, voiceId):
    url = "https://play.ht/api/v1/convert/"
    payload = {
        "content": [text],
        "speed": 1,
        "preset": "balanced",
        "voice": voiceId
    }
    headers = {
        "accept": "application/json",
        "content-type": "application/json",
        "X-User-Id": PLAYHT_ID,
        "Authorization": PLAYHT_KEY
    }

    response = requests.post(url, json=payload, headers=headers)
    print(response.text)
    responseData = json.loads(response.text)
    print(responseData["transcriptionId"])
    return responseData["transcriptionId"]



def push_audio_track(url, audio_data, samplerate, instance_name):
    """
    This function pushes the whole audio track at once via PushAudioRequest()
    PushAudioRequest parameters:
     * audio_data: bytes, containing audio data for the whole track, where each sample is encoded as 4 bytes (float32)
     * samplerate: sampling rate for the audio data
     * instance_name: prim path of the Audio2Face Streaming Audio Player on the stage, were to push the audio data
     * block_until_playback_is_finished: if True, the gRPC request will be blocked until the playback of the pushed track is finished
    The request is passed to PushAudio()
    """

    block_until_playback_is_finished = True  # ADJUST
    with grpc.insecure_channel(url) as channel:
        stub = audio2face_pb2_grpc.Audio2FaceStub(channel)
        request = audio2face_pb2.PushAudioRequest()
        request.audio_data = audio_data.astype(np.float32).tobytes()
        request.samplerate = samplerate
        request.instance_name = instance_name
        request.block_until_playback_is_finished = block_until_playback_is_finished
        print("Sending audio data...")
        response = stub.PushAudio(request)
        if response.success:
            print("SUCCESS")
        else:
            print(f"ERROR: {response.message}")
    print("Closed channel")


def push_audio_track_stream(url, audio_data, samplerate, instance_name):
    """
    This function pushes audio chunks sequentially via PushAudioStreamRequest()
    The function emulates the stream of chunks, generated by splitting input audio track.
    But in a real application such stream of chunks may be aquired from some other streaming source.
    The first message must contain start_marker field, containing only meta information (without audio data):
     * samplerate: sampling rate for the audio data
     * instance_name: prim path of the Audio2Face Streaming Audio Player on the stage, were to push the audio data
     * block_until_playback_is_finished: if True, the gRPC request will be blocked until the playback of the pushed track is finished (after the last message)
    Second and other messages must contain audio_data field:
     * audio_data: bytes, containing audio data for an audio chunk, where each sample is encoded as 4 bytes (float32)
    All messages are packed into a Python generator and passed to PushAudioStream()
    """

    chunk_size = samplerate // 10  # ADJUST
    sleep_between_chunks = 0.04  # ADJUST
    block_until_playback_is_finished = True  # ADJUST

    with grpc.insecure_channel(url) as channel:
        print("Channel creadted")
        stub = audio2face_pb2_grpc.Audio2FaceStub(channel)

        def make_generator():
            start_marker = audio2face_pb2.PushAudioRequestStart(
                samplerate=samplerate,
                instance_name=instance_name,
                block_until_playback_is_finished=block_until_playback_is_finished,
            )
            # At first, we send a message with start_marker
            yield audio2face_pb2.PushAudioStreamRequest(start_marker=start_marker)
            # Then we send messages with audio_data
            for i in range(len(audio_data) // chunk_size + 1):
                time.sleep(sleep_between_chunks)
                chunk = audio_data[i * chunk_size : i * chunk_size + chunk_size]
                yield audio2face_pb2.PushAudioStreamRequest(audio_data=chunk.astype(np.float32).tobytes())

        request_generator = make_generator()
        print("Sending audio data...")
        response = stub.PushAudioStream(request_generator)
        if response.success:
            print("SUCCESS")
        else:
            print(f"ERROR: {response.message}")
    print("Channel closed")


def sendToAudioInstance(audio_fpath, instance_name, sleep_time, url):
    data, samplerate = soundfile.read(audio_fpath, dtype="float32")

    # Only Mono audio is supported
    if len(data.shape) > 1:
        data = np.average(data, axis=1)

    print(f"Sleeping for {sleep_time} seconds")
    time.sleep(sleep_time)

    if 0:  # ADJUST
        # Push the whole audio track at once
        push_audio_track(url, data, samplerate, instance_name)
    else:
        # Emulate audio stream and push audio chunks sequentially
        push_audio_track_stream(url, data, samplerate, instance_name)

def getAllResponds(startText, respondedTextLs):
    crtText = startText
    previousText = "Morty:"+startText
    for i in range(len(respondedTextLs)):
        crtPrompt, toBeCompleteText = getPromptFromText(previousText)
        respondedText = get_completion(crtPrompt, model_id)
        print(respondedText)
        previousText = toBeCompleteText + respondedText
        respondedTextLs[i] = respondedText
    print(respondedTextLs)

def getAudioFile(respondedLs, audioFileLs, fileIndex):
    while(respondedLs[fileIndex]) == None:
        time.sleep(0.2)
    taskId = ""
    if fileIndex%2 == 0:
        taskId = getAudioFromText(respondedLs[fileIndex], rick_id)
    else:
        taskId = getAudioFromText(respondedLs[fileIndex], morty_id)
    audioUrl = getAudioFileFromId(taskId)
    filePath = downloadFile(audioUrl, "audio" + str(fileIndex) + ".wav")
    audioFileLs[fileIndex] = filePath
    return None

def getAllAudioFiles(respondedTextLs, audioFileLs):
    threads = list()
    for i in range(len(respondedTextLs)):
        x = threading.Thread(target = getAudioFile,
                             args=(respondedTextLs, audioFileLs, i))
        threads.append(x)
        x.start()
    
    for i in range(len(threads)):
        thread = threads[i]
        logging.info("Download Audio    : before joining thread %d.", i)
        thread.join()
        logging.info("Downlaod Audio    : thread %d done", i)
    print(audioFileLs)

def playAudioFiles(audioFileLs, instanceA, instanceB, sleep_time, url):
    for i in range(len(audioFileLs)):
        print(i)
        while(audioFileLs[i] == None):
            time.sleep(0.2)
        if i%2 == 0:
            sendToAudioInstance(audioFileLs[i], instanceA, sleep_time, url)
        else:
            sendToAudioInstance(audioFileLs[i], instanceB, sleep_time, url)
        crtDuration = librosa.get_duration(path=audioFileLs[i])
        
        print(crtDuration)
        # time.sleep(crtDuration+0.1)


def main():
    """
    This demo script shows how to send audio data to Audio2Face Streaming Audio Player via gRPC requests.
    There two options:
     * Send the whole track at once using PushAudioRequest()
     * Send the audio chunks seuqntially in a stream using PushAudioStreamRequest()
    For the second option this script emulates the stream of chunks, generated by splitting an input WAV audio file.
    But in a real application such stream of chunks may be aquired from some other streaming source:
     * streaming audio via internet, streaming Text-To-Speech, etc
    gRPC protocol details could be find in audio2face.proto
    """

    if len(sys.argv) < 3:
        print("Format: python test_client.py PATH_TO_WAV INSTANCE_NAME")
        return
    
    startText = sys.argv[3]
    instance_A_name = sys.argv[1]
    instance_B_name = sys.argv[2]

    # Sleep time emulates long latency of the request
    sleep_time = 0.5  # ADJUST

    # URL of the Audio2Face Streaming Audio Player server (where A2F App is running)
    url = "localhost:50051"  # ADJUST

    loopNum = 4
    respondedTextLs = [None]*loopNum * 2
    audioFileLs = [None]*loopNum*2
    print("start")
    respondThread = threading.Thread(target=getAllResponds, args=(startText, respondedTextLs))
    respondThread.start()
    print(respondedTextLs)
    audioThread = threading.Thread(target=getAllAudioFiles, args=(respondedTextLs, audioFileLs))
    audioThread.start()
    # audioFileLs = ["audio0.wav", "audio1.wav", "audio2.wav", "audio3.wav"]
    time.sleep(30)
    playAudioFiles(audioFileLs, instance_A_name, instance_B_name, sleep_time, url)

        

        

if __name__ == "__main__":
    main()
